{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T08:00:32.002163816Z",
     "start_time": "2023-09-25T08:00:31.986949398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python c:\\Users\\ACERPC\\Desktop\\robosuite\\robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    }
   ],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.wrappers.gym_wrapper import GymWrapper\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cac3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "new_folder = os.path.join(cwd, \"tmp/gym\")\n",
    "os.makedirs(new_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f26bfc569a1dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T06:25:12.970212130Z",
     "start_time": "2023-09-25T06:25:11.779955106Z"
    }
   },
   "outputs": [],
   "source": [
    "# create environment instance\n",
    "env = GymWrapper(suite.make(\n",
    "    env_name=\"Lift\", # try with other tasks like \"Stack\" and \"Door\"\n",
    "    robots=\"UR5e\",  # try with other robots like \"Sawyer\" and \"Jaco\"\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=True,\n",
    "    use_object_obs=True,                   # don't provide object observations to agent\n",
    "    use_camera_obs=True,\n",
    "    camera_names=\"robot0_eye_in_hand\",      # use \"agentview\" camera for observations\n",
    "    camera_heights=84,                      # image height\n",
    "    camera_widths=84,                       # image width\n",
    "    reward_shaping=True,                    # use a dense reward signal for learning\n",
    "    horizon = 500,\n",
    "    control_freq=20,                        # control should happen fast enough so that simulation looks smooth\n",
    "), ['robot0_eye_in_hand_image'])\n",
    "env = Monitor(env, \"tmp/gym\" )\n",
    "# ), ['robot0_eye_in_hand_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39abde0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('robot0_eye_in_hand_image': Box(0, 255, (84, 84, 3), uint8))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe79fc835d0d5d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T06:48:04.884357Z",
     "start_time": "2023-09-25T06:27:32.303421583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You must use `MultiInputPolicy` when working with dict observation space, not CnnPolicy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\rb_custom.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39;49m\u001b[39mCnnPolicy\u001b[39;49m\u001b[39m\"\u001b[39;49m, env, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, progress_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log_interval\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\anaconda3\\envs\\robo_custom\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:104\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     79\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     _init_setup_model: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m ):\n\u001b[1;32m--> 104\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    105\u001b[0m         policy,\n\u001b[0;32m    106\u001b[0m         env,\n\u001b[0;32m    107\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    108\u001b[0m         n_steps\u001b[39m=\u001b[39;49mn_steps,\n\u001b[0;32m    109\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[0;32m    110\u001b[0m         gae_lambda\u001b[39m=\u001b[39;49mgae_lambda,\n\u001b[0;32m    111\u001b[0m         ent_coef\u001b[39m=\u001b[39;49ment_coef,\n\u001b[0;32m    112\u001b[0m         vf_coef\u001b[39m=\u001b[39;49mvf_coef,\n\u001b[0;32m    113\u001b[0m         max_grad_norm\u001b[39m=\u001b[39;49mmax_grad_norm,\n\u001b[0;32m    114\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m    115\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m    116\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m    117\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m    118\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m    119\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    120\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    121\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    122\u001b[0m         _init_setup_model\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    123\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49m(\n\u001b[0;32m    124\u001b[0m             spaces\u001b[39m.\u001b[39;49mBox,\n\u001b[0;32m    125\u001b[0m             spaces\u001b[39m.\u001b[39;49mDiscrete,\n\u001b[0;32m    126\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiDiscrete,\n\u001b[0;32m    127\u001b[0m             spaces\u001b[39m.\u001b[39;49mMultiBinary,\n\u001b[0;32m    128\u001b[0m         ),\n\u001b[0;32m    129\u001b[0m     )\n\u001b[0;32m    131\u001b[0m     \u001b[39m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[39m# because of the advantage normalization\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\anaconda3\\envs\\robo_custom\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:81\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     60\u001b[0m     policy: Union[\u001b[39mstr\u001b[39m, Type[ActorCriticPolicy]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     supported_action_spaces: Optional[Tuple[Type[spaces\u001b[39m.\u001b[39mSpace], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m ):\n\u001b[1;32m---> 81\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     82\u001b[0m         policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m     83\u001b[0m         env\u001b[39m=\u001b[39;49menv,\n\u001b[0;32m     84\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m     85\u001b[0m         policy_kwargs\u001b[39m=\u001b[39;49mpolicy_kwargs,\n\u001b[0;32m     86\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m     87\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m     88\u001b[0m         use_sde\u001b[39m=\u001b[39;49muse_sde,\n\u001b[0;32m     89\u001b[0m         sde_sample_freq\u001b[39m=\u001b[39;49msde_sample_freq,\n\u001b[0;32m     90\u001b[0m         support_multi_env\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     91\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m     92\u001b[0m         stats_window_size\u001b[39m=\u001b[39;49mstats_window_size,\n\u001b[0;32m     93\u001b[0m         tensorboard_log\u001b[39m=\u001b[39;49mtensorboard_log,\n\u001b[0;32m     94\u001b[0m         supported_action_spaces\u001b[39m=\u001b[39;49msupported_action_spaces,\n\u001b[0;32m     95\u001b[0m     )\n\u001b[0;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps \u001b[39m=\u001b[39m n_steps\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m=\u001b[39m gamma\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\anaconda3\\envs\\robo_custom\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:192\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[1;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m# Catch common mistake: using MlpPolicy/CnnPolicy instead of MultiInputPolicy\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m policy \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCnnPolicy\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, spaces\u001b[39m.\u001b[39mDict):\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou must use `MultiInputPolicy` when working with dict observation space, not \u001b[39m\u001b[39m{\u001b[39;00mpolicy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_sde \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    195\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgeneralized State-Dependent Exploration (gSDE) can only be used with continuous actions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: You must use `MultiInputPolicy` when working with dict observation space, not CnnPolicy"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=2048, progress_bar=True, log_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650999e57ee19c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T08:01:01.175729695Z",
     "start_time": "2023-09-25T08:00:36.924182236Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\rb_custom.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m action, state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m obs, reward, done,done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ACERPC/Desktop/robosuite/rb_custom.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\anaconda3\\envs\\robo_custom\\lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RenderFrame \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[RenderFrame] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\robosuite\\wrappers\\wrapper.py:71\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m    By default, run the normal environment render() function\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m        **kwargs (dict): Any args to pass to environment render function\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\robosuite\\environments\\base.py:450\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    447\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39m    Renders to an on-screen window.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mviewer\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\robosuite\\utils\\opencv_renderer.py:29\u001b[0m, in \u001b[0;36mOpenCVRenderer.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     28\u001b[0m     \u001b[39m# get frame with offscreen renderer (assumes that the renderer already exists)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     im \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mrender(camera_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcamera_name, height\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheight, width\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwidth)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     31\u001b[0m     \u001b[39m# write frame to window\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     im \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mflip(im, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\robosuite\\utils\\binding_utils.py:1133\u001b[0m, in \u001b[0;36mMjSim.render\u001b[1;34m(self, width, height, camera_name, depth, mode, device_id, segmentation)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m _MjSim_render_lock:\n\u001b[0;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_context_offscreen\u001b[39m.\u001b[39mrender(\n\u001b[0;32m   1131\u001b[0m         width\u001b[39m=\u001b[39mwidth, height\u001b[39m=\u001b[39mheight, camera_id\u001b[39m=\u001b[39mcamera_id, segmentation\u001b[39m=\u001b[39msegmentation\n\u001b[0;32m   1132\u001b[0m     )\n\u001b[1;32m-> 1133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_context_offscreen\u001b[39m.\u001b[39;49mread_pixels(width, height, depth\u001b[39m=\u001b[39;49mdepth, segmentation\u001b[39m=\u001b[39;49msegmentation)\n",
      "File \u001b[1;32mc:\\Users\\ACERPC\\Desktop\\robosuite\\robosuite\\utils\\binding_utils.py:173\u001b[0m, in \u001b[0;36mMjRenderContext.read_pixels\u001b[1;34m(self, width, height, depth, segmentation)\u001b[0m\n\u001b[0;32m    170\u001b[0m rgb_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((height, width, \u001b[39m3\u001b[39m), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39muint8)\n\u001b[0;32m    171\u001b[0m depth_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((height, width), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32) \u001b[39mif\u001b[39;00m depth \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m mujoco\u001b[39m.\u001b[39;49mmjr_readPixels(rgb\u001b[39m=\u001b[39;49mrgb_img, depth\u001b[39m=\u001b[39;49mdepth_img, viewport\u001b[39m=\u001b[39;49mviewport, con\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcon)\n\u001b[0;32m    175\u001b[0m ret_img \u001b[39m=\u001b[39m rgb_img\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m segmentation:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs = env.reset()[0]\n",
    "for i in range(500):\n",
    "    action, state = model.predict(obs)\n",
    "    obs, reward, done,done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.2)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6e88f34c3e1ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T07:45:52.507218709Z",
     "start_time": "2023-09-25T07:19:44.084127030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0d0b5f52c94a4a92ccf9d336e32bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 74.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 89         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 227        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49315557 |\n",
      "|    clip_fraction        | 0.723      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.56      |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0686    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0664    |\n",
      "|    std                  | 0.401      |\n",
      "|    value_loss           | 0.785      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 500       |\n",
      "|    ep_rew_mean          | 80.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 20        |\n",
      "|    time_elapsed         | 460       |\n",
      "|    total_timesteps      | 40960     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6722676 |\n",
      "|    clip_fraction        | 0.759     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.91     |\n",
      "|    explained_variance   | 0.875     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 1660      |\n",
      "|    policy_gradient_loss | -0.0735   |\n",
      "|    std                  | 0.365     |\n",
      "|    value_loss           | 0.442     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 500       |\n",
      "|    ep_rew_mean          | 90.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 30        |\n",
      "|    time_elapsed         | 697       |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7316275 |\n",
      "|    clip_fraction        | 0.781     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.33     |\n",
      "|    explained_variance   | 0.921     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0569   |\n",
      "|    n_updates            | 1760      |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    std                  | 0.337     |\n",
      "|    value_loss           | 0.464     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 500       |\n",
      "|    ep_rew_mean          | 91        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 88        |\n",
      "|    iterations           | 40        |\n",
      "|    time_elapsed         | 930       |\n",
      "|    total_timesteps      | 81920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0585983 |\n",
      "|    clip_fraction        | 0.762     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.77     |\n",
      "|    explained_variance   | 0.965     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0505   |\n",
      "|    n_updates            | 1860      |\n",
      "|    policy_gradient_loss | -0.0706   |\n",
      "|    std                  | 0.311     |\n",
      "|    value_loss           | 0.372     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "model = PPO.load('trained_model', env=env)\n",
    "model.learn(total_timesteps=100000, progress_bar=True, log_interval=10)\n",
    "model.save('trained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e997e532ad5e1641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32703687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
